RCA - Network Infrastructure Impacting Multiple Services - Australia East
Summary of impact: Between 1:48 and 5:15 UTC on June 11, 2017, a subset of customers may have encountered reachability failures for some of their resources in Australia East. Starting at 1:48 UTC, outbound network connectivity in the region was interrupted for a subset of customers, which caused DNS lookups and Storage connections to fail and as a result some customer Virtual Machines shut down and restarted. Engineers investigated and found that a deployment of a new instance of the Azure Load Balancer service was underway in the region. At 3:40 UTC, the new deployment was disabled, after which customer impact for networking services was mitigated. A subset of impacted services experienced a delayed recovery, and engineers conﬁrmed all services were fully mitigated at 5:15 UTC. Deployment activities were paused until a full RCA could be completed, and necessary repairs resolved. Customer impact: Due to outbound connectivity failure, the impacted services which are dependent on resources internal and/or external to Azure would have seen reachability failures for the duration of this incident. Customer Virtual Machines would have shut down as designed after losing connectivity to the correlating storage services. A subset of customers would have experienced the following impact for affected services: Redis Cache - unable to connect to Redis caches Azure Search - search service unavailable Azure Media Services - media streaming failures App Services - availability close to 0% Backup - backup management operation failures Azure Stream Analytics - cannot create jobs Azure Site Recovery - protection and failover failures Visual Studio Team Services - connection failures Azure Key Vault - connection failures Root cause and mitigation: A deployment of conﬁguration settings for the Azure Load Balancer service caused an outage when a component of the new deployment began advertising address ranges already advertised by an existing instance of the Azure Load Balancer service. The new deployment was a shadow environment speciﬁcally conﬁgured to be inactive (and hence, not to advertise any address ranges) in terms of customer trafﬁc. Because the route advertisement from this new service was more speciﬁc than the existing route advertisement, trafﬁc was diverted to the new inactive deployment. When this occurred, outbound connectivity was interrupted for a subset of services in the region. The event was triggered by a software bug within the new deployment. The new deployment was passively ingesting state from the existing deployment and validating it. A conﬁguration setting was in place to prevent this ingested state to go beyond validation and result in route advertisement. Due the software bug, this conﬁguration setting failed. The software bug was not detected in deployments in prior regions because it only manifested under speciﬁc combinations of the services own address range conﬁguration. Australia East was the ﬁrst time this combination occurred. Next steps: We sincerely apologize for the impact to affected customers. We are continuously taking steps to improve the Microsoft Azure Platform and our processes to help ensure such incidents do not occur in the future. In this case, we will: 1. Repair the software bug to ensure address ranges marked as DO NOT advertise are NOT advertised. 2. Implement additional automatic rollback capabilities of deployments based on health probe for data plane impacting deployments (control plane impacting deployments are already automatically rolled back). 3. Ensure all Azure Load Balancer deployments pass address-range health checks before reaching production. Provide feedback: Please help us improve the Azure customer communications experience by taking our survey https://survey.microsoft.com/387244 