RCA for Storage in North Europe affecting Multiple Services 
Summary of impact: Between 13:22 and 16:10 UTC on 07 December 2016, a subset of customers using Storage services in North Europe may have experienced difficulties whilst attempting to connect to their resources. In addition, customers using Virtual Machines (VMs) hosted in the affected region may have experienced VM restarts. App Service \ Web App and Azure Search customers that have a dependency on Storage services may also have experienced availability issues in this region. The issue was due to a software bug in an extent management process of Storage services that resulted in the inability to process requests in a scale unit. Azure engineering received VM failures alerts, identified the issue and applied a temporary mitigation by failing over the affected processes. This restored the system to a healthy state. A further subset of customers were identified who may have required additional steps to fully restore system health after VM reboots. Affected customers were notified via their management portal and were given instructions to follow the article https://aka.ms/vmrecovery to restore service health. We sincerely apologize for the extended impact to affected customers. Workaround: During the impacted timeframe, retries may have succeeded for some customers. Root cause and mitigation: The Azure Storage system writes data to extents in a three replica format to ensure high redundancy of the data. Each replica is stored on a separate node, in a separate rack to ensure isolation. The system is designed to ensure that no more than one replica is taken offline at a time for regular platform maintenance, such as software upgrades. Nodes in our datacenter typically have a low failure rate and the system is designed to maintain data availability in the event of unexpected failures. When a disk or node fails, the replication system recognizes it, and replicates the data elsewhere in order to maintain data durability. Though there is significant redundancy in the Storage service subsystem, there is active monitoring to detect impact across all three replicas. This monitoring is designed to trigger alerts that notify engineers to immediately initiate recovery methods on the unavailable replica. The system then enters into failsafe mode resulting in a pause of any data deletion actions on data tagged for deletion (Garbage Collection) until the replicas are again available, the system exits failsafe mode, and the service is fully recovered. 
Additionally, the system has designed safeguards which control capacity of affected Storage scale unit if the system goes in extended failsafe mode operation by unpausing garbage collection. In this incident, a software bug in a storage unit reported a false positive for the above mentioned monitoring, in turn triggering the system to operate in failsafe mode. This software bug also suppressed the expected alerting from this monitoring, in turn not raising any warning to engineers to engage for recovery. Consequently, the system was operating in failsafe mode for an extended period. This should have automatically triggered the designed safeguard mechanism to free up space by a garbage collection process to maintain capacity. However in this incident the designed safeguard mechanism didn’t start as expected, thus resulting in eventual running out of space in some of nodes. 
Overall capacity in a scale unit was well below safeguard threshold although many individual nodes were almost full. We have a process that manages extents on each storage node that constantly communicates to a metadata server and signs up to take customer write requests based on available disk space. In a normal operation, a metadata server would have detected nodes with no available disk space and prevented accepting further write traffic. In this incident a software bug in an extent management process resulted in the incorrect reporting of available disk space and in turn, kept write traffic diverted to itself. Since these nodes didn’t have enough disk space to serve write traffic, this eventually resulted an extent management process failure. Once the process recovered from a failure, receiving again incorrect report of available disk space, this resulted in getting more write traffic assigned to itself and failing to serve write operation then again experiencing a failure. This cycle kept repeating itself for a portion of write requests to this scale unit. As a result of this, a subset storage accounts data was temporary unavailable and in turn causing IaaS Virtual Machines to crash. 
Failures of IaaS Virtual Machines raised alarms in our system and engineers were engaged for manual recovery of the system. As soon as engineers suppressed the false positive monitoring, the system exited failsafe mode and returned to normal operation. Normal garbage collection process was resumed as well. This freed up space on all of affected nodes and relieved the system from capacity pressure and in turn returning to normal operation where all write requests were successfully fulfilled. Next steps: We sincerely apologize for the impact to affected customers. We are continuously taking steps to improve the Microsoft Azure Platform and our processes to help ensure such incidents do not occur in the future. In this case, this includes (but is not limited to): 
1. Fix a software bug in the extent management process of Storage service. A fix has been rolled out to production – completed. 2. Review and assess data loss alerting and safe guard thresholds to cover similar scenarios. 3. Review recovery processes to be able to minimize the time to recovery and automate where it is possible. Provide feedback: Please help us improve the Azure customer communications experience by taking our survey https://survey.microsoft.com/244454
