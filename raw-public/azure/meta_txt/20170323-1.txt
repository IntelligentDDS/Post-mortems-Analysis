RCA – Data Lake Analytics and Data Lake Store – East US 2 
Summary of impact: Between approximately March 22 15:02 UTC and March 22 17:25 UTC, a subset of customers experienced a number of intermittent failures when accessing Azure Data Lake Store (ADLS) in East US 2. This may have caused Azure Data Lake Analytics job and/or ADLS request failures. This incident was because of a misconfiguration which resulted in one of the ADLS microservices not serving the requests for the customers. Azure Monitoring detected the event and an alert was triggered. Azure engineers engaged immediately and mitigated the issue by reverting the configuration that triggered the incident. Customer impact: A subset of customers in East US 2 had intermittent failures when accessing their Azure Data Lake Service during the timeframe mentioned above. Workaround: Retry failed operation. If a customer uses Azure Data Lake Store Java SDK 2.1.4 to access the service, the SDK would have automatically retried and some of the requests may have succeeded with higher latencies. Root cause and mitigation: A misconﬁguration in one of the microservices that ADLS depends on caused the ADLS microservices to not serve requests after they, or the instances they resided on restarted (which restart is part of regular maintenance process). As more instances were restarted, there were fewer instances of the affected ADLS microservice remaining to serve the requests, which resulted in high latencies and consequently requests failing. Azure engineers identiﬁed and reverted the conﬁguration at fault. As the conﬁguration ﬁx was propagated as an expedited ﬁx, and the services were restarted, the affected service instances started to serve requests again, which mitigated this issue. This resulted in intermittent Azure Data Lake Analytics job and ADLS request failures. The jobs that were submitted during the incident are expected to have been queued and ran after the incident, which would have resulted in delays for the start of the jobs.
Was the issue detected? The issue was detected by our telemetry. An alert was raised, which prompted Azure engineers to investigate an Azure Data Lake Store issue and mitigate the issue.
To achieve quickest possible notiﬁcation of Service Events to our customers, the Azure infrastructure has a framework that automates the stream from alert to Service Health Dashboard and/or Azure Portal Notiﬁcations. Unfortunately, this class of alert does not contain the needed correlation for automation now.  We did surface this outage via the Resource Health feature with customer’s ADLA and ADLS account(s) in this region.  We will continue to implement notiﬁcation automations as well as ensuring manual communications protocols are followed quickly as possible. In this incident, the issue was announced on Service Health Dashboard manually. Next steps: We sincerely apologize for the impact to affected customers. We are continuously taking steps to improve the Microsoft Azure Platform and our processes to help ensure such incidents do not occur in the future. In this case, this includes (but is not limited to): • Fix the misconfiguration. [Completed] • Improve testing by adding more validations for this sort of configuration error to prevent future occurrences. [Completed] • Separate the defined configuration container such that it is limiting the impact of future occurrences to smaller slices of the service. [In progress] • Improve the format of the configuration file to make the code review more efficient in the future. [In progress] • Add alerts deeper in the stack such that there is earlier/redundant indications of a similar problem for faster debugging. [In progress] • Rearchitect the initialization phase of the affected service to reduce the dependency on the service that was originally impacted. [Long term] Provide feedback: Please help us improve the Azure customer communications experience by taking our survey https://survey.microsoft.com/346393
