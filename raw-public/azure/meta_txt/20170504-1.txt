RCA - Virtual Machines - Japan East 
Summary of impact: Between 20:55 and 21:35 UTC on 04 May 2017, a subset of customers using Virtual Machines in Japan East may have experienced intermittent connection failures when trying to access Virtual Machines in the region. Some Virtual Machines may have also restarted unexpectedly. Engineers detected alerts and engaged promptly. The issue was self-healed by the Azure platform self-heal mechanism, Engineers continued to investigate to establish the full root cause and implemented preventive measures to avoid future occurrences. Customer impact: A subset of customers using Virtual Machines in Japan East may have experienced intermittent connection failures when trying to access Virtual Machines in the region. Some Virtual Machines may have also restarted unexpectedly. A subset of customers using Backup in Japan East may have received timeout error notifications when performing backup and restore operations. Root cause and mitigation: The Azure Storage system writes data to extents in a 3 replica format to ensure high availability and durability of the data. Each replica is stored on a separate node, in a separate rack. Care is taken to ensure that no more than one replica is taken offline at a time for regular platform maintenance such as software upgrades or hardware repairs. Nodes in our datacenter typically have a low failure rate and the system is designed to maintain data availability in the event of unexpected failures. When a disk or node fails, the replication system recognizes it, and replicates the data elsewhere in order to maintain data durability. During the incident window, this storage cluster had an abnormally high percentage of nodes out for HW repairs, and was undergoing a software update deployment. The storage system is designed to handle some storage nodes being down for repairs and maintenance without any impact on data availability or customer experience. Unfortunately in this incident the combination of the above and a higher than normal request load on the cluster, exposed a rare software bug which resulted in the failure of storage roles on several additional nodes. This resulted in some data extents becoming temporarily unavailable to clients, including VMs relying on those extents. The VM failures raised alerts in our system and engineers were engaged. The failed storage roles recovered automatically, but engineers took additional action to reduce the overall load on the cluster. Storage returned to normal operation and VMs were recovered. Next steps: We sincerely apologize for the impact to the affected customers. We are continuously taking steps to improve the Microsoft Azure Platform and our processes to help ensure such incidents do not occur in the future, and in this case it includes (but is not limited to): 
1. Adjustments are being made to the deployment pipeline to further increase the existing safety margin requirement for deployments 2. A fix for the rare software bug is being engineered and will roll out as soon as possible while observing safe deployment process. Provide feedback: Please help us improve the Azure customer communications experience by taking our survey https://survey.microsoft.com/361159
