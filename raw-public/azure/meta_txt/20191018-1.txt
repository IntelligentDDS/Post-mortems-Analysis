RCA - Authentication issues with Azure MFA in North America (Tracking ID 1TP8-PS0)
Summary of impact: Between 13:30 UTC and 15:57 UTC on 18 October 2019, customers in North America experienced issues receiving multi-factor authentication (MFA) challenges. Users who had valid MFA claims during the incident were not impacted. However, users who were required to perform an MFA challenge during this incident were unable to complete the challenge. This represented 0.51% of users in North American tenants using the service during the incident.
Root cause: At 13:30 UTC, severe packet loss was experienced on an external network route between Microsoft and the Apple Push Notification Service (APNS). The packet loss was greater in severity and duration than previously encountered. It also coincided with morning peak traffic in North America. This combination of events caused a build-up of unprocessed requests in the MFA service, leading to service degradation and failures in MFA requests.
Mitigation: Service monitors detected the build-up of unprocessed requests in the MFA service at 13:38 UTC and engineers were fully engaged by 13:48 UTC. Engineering confirmed the issue was a loss of network connectivity and began troubleshooting within the Microsoft datacenter networks. Engineering determined that the datacenter networks did not experience loss of connectivity and pinpointed the issue as external to the Microsoft datacenter networks. While further troubleshooting was underway to identify the most impacted network routes, engineering prepared a hotfix to bypass the impacted external service altogether, and to restore MFA functionality. The hotfix was rolled out to one region to validate the effectiveness of the fix. In the meantime, the external network recovered, and packet loss was reduced to normal rates. Engineering paused further rollout of the hotfix. The network issue was confirmed to be mitigated at 15:57 UTC, and the MFA service functionality recovered. The hotfix, which was then redundant, was rolled back.
Following mitigation, engineering immediately performed the below tasks to harden the service against a similar degradation in the future:
The impacted network path was pinpointed, and traffic was reconfigured to bypass the failure path.
Three hotfixes to the service were immediately rolled out to rebalance network traffic and increase resiliency to network failures Precise alerts were setup to monitor for the specific network condition which triggered the issue.
Next steps: We sincerely apologize for the impact to affected customers. We are continuously taking steps to improve the Microsoft Azure platform and our processes to help ensure such incidents do not occur in the future, including, but not limited to:
1. Fine-grained fault domain isolation work has been accelerated. This work builds on the previous fault domain isolation work which limited this incident to North American tenants. This includes:
Additional physical partitioning within each Azure region. Logical partitioning between authentication types. Improved partitioning between service tiers.
2. Additional hardening and redundancy within each granular fault domain to make them more resilient to network connectivity loss. This includes:
Improved resilience to request build-up.
Optimizing network traffic to decrease load on network links.
Improved instructions to users for self-service in case notifications are not delivered. Service restructuring to decrease service impact of network packet loss.
3. Enhanced monitoring for networking latency and various resource usage thresholds. This includes:
Multi-region and multi-cloud targeted monitoring for the specific type of packet loss encountered. Improved monitors for additional types of resource usage.
Other categories and action items are being actively identified.
Provide feedback: Please help us improve the Azure customer communications experience by taking our survey https://aka.ms/1TP8-PS0