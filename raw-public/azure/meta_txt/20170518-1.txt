RCA - Storage Latency - West Europe
Summary of impact: Between 02:45 and 15:59 UTC on 18 May 2017, with most impact occurring between 07:00 and 13:00 UTC, a subset of customers using Virtual Machines (VMs), Storage, HDInsight, or Azure Search in West Europe may have experienced high latency or degraded performance when accessing resources hosted in this region. A subset of customers using VMs in the region may have experienced unexpected restarts of their VM. Engineers investigated and determined that root cause was due to congested links in our network infrastructure resulting in reduced network capacity and subsequent impacts to downstream services. Customer impact: A subset of customers using VMs instances may have experienced unavailability or connection errors due to Storage account accessibility issues. Customers could have seen VMs were either unavailable or having connectivity issues. After mitigation, Storage account access should have been restored. Root cause and mitigation: Microsoft Azure has implemented an automated service, Lossy Link Monitor (LLM), to eliminate lossy links that monitors all our network interfaces and links for intermittent failures and automatically performs remediation when errors are detected. LLM operates by identifying links that appear to be dropping packets; determining that it would be safe to turn off the link; issuing commands to the switch to turn off the BGP (Border Gateway Protocol) session that directs traffic over the link; opening tickets with on-site staff to have the links repaired; and then restoring traffic to the links after they are repaired and verified working. LLM performs remediation within capacity and error rate thresholds. To increase capacity on aggregation routers, a routine maintenance was performed to deploy configuration updates. This resulted in increased link errors on adjacent downstream devices. LLM detected these link errors and shut down the links. Consequently, there was an unexpected reduction of network capacity for a single data center in the region, causing latency and increased packet-loss for up to 1/8th of flows into and out of the data center. The level of impact varied significantly during the incident period due to variation in the amount of traffic being carried in the data center and the progress of restoring capacity. While engineers worked towards mitigation, Microsoft ownedservices reduced their offered traffic load to provide more headroom to Microsoft customer traffic. Engineers disabled the repair service and re-enabled all links that were verified to be carrying traffic correctly, eliminating network congestion and mitigating the issue. RECOMMENDATION (s): Customers may choose to leverage Availability Sets to provide additional redundancy for their application. To learn more about this and other best practices for application resiliency, please refer to the Resiliency checklist at the following link: https://aka.ms/d_nyzbkrl8. Azure Advisor also provides personalized HA recommendations to improve application availability. Please refer to the following link for additional information on Azure Advisor: https://aka.ms/x3aqmn Next steps: We sincerely apologize for the impact to affected customers. We are continuously taking steps to improve the Microsoft Azure Platform and our processes to help ensure such incidents do not occur in the future. In this case, this includes (but is not limited to): 1. Update the automated monitoring and repair service to account for maintenance scenarios where higher than normal link errors are expected (in progress) 2. Improve alarming for reduced capacity workflows (in progress) 3. Improve telemetry to maintain and leverage accurate state of Networking devices (in progress) Provide feedback: Please help us improve the Azure customer communications experience by taking our survey https://survey.microsoft.com/365437
