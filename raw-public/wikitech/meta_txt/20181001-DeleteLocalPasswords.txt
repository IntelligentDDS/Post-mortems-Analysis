The MediaWiki maintenance script (T201009 / T201009, DeleteLocalPasswords.php) did not include a call. When run on certain databases (commons and dewiki), it updated over a million rows, causing an extended period read only time and slowdown of requests on Commons (and other projects using Commons database resources) approximately between 10:48 and 11:01 and on the German Wikipedia between 11:02 and 11:16 UTC.

SAL for 2018-10-01: Script starts (this wasn't known at the time of the initial response)

But other hosts starts alerting, too:

Swat is paused, probably a master or code bug issue is thought. Main queries happening at the moment from replication are DeleteLocalPassword ones from mwmaint2001. Discussion happens if to kill the commons script, and it is done (but the issue just jumps into dewiki).

dewiki ongoing process is killed at the same time, causing temporary lag, too (but not enough to page):
• Do not forget to add calls.
• Registering the maintenance on the Deployments page might have helped to identify the issue more quickly.
• The script ran successfully on fawiki (medium-sized). Is there some way to notice that a script is causing replag (assuming it did so at all) when it is not large enough to bring things down?

Not sure what would be relevant here. waitForReplication()?
• Consider adding code to the MediaWiki DB layer to detect huge numbers of writes without waiting for slaves - phab:T205893
• Consider adding dry-run options on all maintenance scripts, even those that seem trivial in the future
• Communicate more clearly to sysadmins (people that can respond quickly to incidents) of what, where and when maintenance scripts are done as documented at https://wikitech.wikimedia.org/wiki/Deployments (Long running changes/scripts) so the script can be easily and faster killed if an unexpected issue arises